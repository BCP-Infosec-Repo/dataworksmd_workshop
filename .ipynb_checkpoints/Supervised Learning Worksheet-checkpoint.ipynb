{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/GTK_Logo_Social Icon.jpg\" align=\"left\" height=125, width=125  style=\"margin:0px 15px\"/> \n",
    "<img src=\"img/dataworks.jpeg\" align=\"right\" height=125, width=125  style=\"margin:0px 15px\"/> \n",
    "\n",
    "# Supervised Learning Worksheet\n",
    "This worksheet covers concepts relating to tuning a classifier.  For this example, we will be using the problem of identifying domains generated by Domain Generating Algorithms (DGA).  \n",
    "\n",
    "Please raise your hand if you get stuck.  \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (http://matplotlib.org/api/pyplot_api.html)\n",
    "* Scikit-learn (http://scikit-learn.org/stable/documentation.html)\n",
    "* YellowBrick (http://www.scikit-yb.org/en/latest/)\n",
    "* Seaborn (https://seaborn.pydata.org)\n",
    "* Lime (https://github.com/marcotcr/lime)\n",
    "* TPOT (https://epistasislab.github.io/tpot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:27.785956Z",
     "start_time": "2020-01-21T17:47:26.816693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scikitplot as skplt\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import lime\n",
    "from tpot import TPOTClassifier\n",
    "import joblib\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "For this exercise, we are going to focus on building a pipeline and then tuning the resultant model, so we're going to use a simple model with only five features.\n",
    "\n",
    "This workshop did not cover feature extraction and engineering, so we will just make your life easy and give you the data.  The features are:\n",
    "* `length`: The character length of the domain.\n",
    "* `digits`: The number of digits in the domain.\n",
    "* `entropy`: The Shannon-entropy of the domain. (https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "* `vowel-cons`: The ratio of vowels to consonants.\n",
    "* `firstDigitIndex`:  The index of the first digit in the domain.  Defaults to zero if no digits are present\n",
    "* `ngrams`: The normalized sum of 2-grams, 3-grams and 4-grams present in the domain.\n",
    "\n",
    "This data set has 1000 of each class which is contained in the column `isDGA`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:27.829268Z",
     "start_time": "2020-01-21T17:47:27.813969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell as is\n",
    "df_final = pd.read_csv('data/dga_features_final_df.csv')\n",
    "target = df_final['isDGA']\n",
    "feature_matrix = df_final.drop(['isDGA'], axis=1)\n",
    "feature_matrix.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets.\n",
    "Next, we're going to need a training and testing dataset, so you know the drill, split the data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:28.396363Z",
     "start_time": "2020-01-21T17:47:28.392100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Cross-Validation: Split the data set into training and test data\n",
    "feature_matrix_train, feature_matrix_test, target_train, target_test = train_test_split(feature_matrix, \n",
    "                                                                                        target, \n",
    "                                                                                        test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model\n",
    "For this exercise, we're going to create a K-NN Classifier for the DGA data and tune it, but first, create a classifier with the default options and calculate the accuracy score for it. (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) \n",
    "\n",
    "The default parameters are shown below.\n",
    "```python \n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
    "           weights='uniform')\n",
    "```           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:29.220463Z",
     "start_time": "2020-01-21T17:47:29.213766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:29.623006Z",
     "start_time": "2020-01-21T17:47:29.606016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the predictions in a variable called default_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:30.017018Z",
     "start_time": "2020-01-21T17:47:30.012816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the accuracy score (HINT accuracy_score()) by comparing the target_test to the default_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Performance \n",
    "Out of the box, the model achieves approximately 85% accuracy.  Better than chance but let's see if we can do better. \n",
    "\n",
    "**Note:  This notebook is written without using fixed random seeds, so you might get slightly different results.**\n",
    "\n",
    "### Scaling the Features\n",
    "K-NN is a distance-based classifier and hence it is necessary to scale the features prior to training the model.  For this exercise however, let's create a simple pipeline with two steps:\n",
    "\n",
    "1.  StandardScaler\n",
    "2.  Train the classifier\n",
    "\n",
    "Pipelines are objects which can encapsulate multiple steps in the ML process.  Here is a link to the documentation (https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and a brief tutorial about pipelines: (https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976)\n",
    "\n",
    "Once you've done that, calculate the accuracy and see if it has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:30.823671Z",
     "start_time": "2020-01-21T17:47:30.821263Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the pipeline here\n",
    "pipeline = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:31.249805Z",
     "start_time": "2020-01-21T17:47:31.237910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now fit the pipeline as you would a regular model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:31.774238Z",
     "start_time": "2020-01-21T17:47:31.754491Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Next, make predictions using the pipeline object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:32.199237Z",
     "start_time": "2020-01-21T17:47:32.195071Z"
    }
   },
   "outputs": [],
   "source": [
    "# And... calculate the accuracy_score by comparing the target_test with the predictions from the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the features did result in a small improvement: .85 accuracy to .88.  But let's see if we can't do even better.\n",
    "\n",
    "### Using RandomSearchCV and GridSearchCV to tune Hyperparameters\n",
    "Now that we've scaled the features and built a simple pipeline, let's try to tune the hyperparameters to see if we can improve the model performance.  Scikit-learn provides two methods for accomplishing this task: `RandomizedSearchCV` and `GridSearchCV`. \n",
    "\n",
    "\n",
    "* `GridSearchCV`:  GridSearch iterates through all possible combinations of tuning parameters to find the optimal combination. (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "* `RandomizedSearchCV`:  RandomizedSearch interates through random combinations of paremeters to find the optimal combination.  While RandomizedSearch does not try every possible combination, is considerably faster than GridSearch and has been shown to get very close to the optimal combination in considerably less time.  (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) \n",
    "\n",
    "You can see in the results below, that the model was able to achieve **91.9%** accuracy with RandomSearch!   \n",
    "```\n",
    "[INFO] randomized search took 0.85 seconds\n",
    "[INFO] grid search accuracy: 91.93%\n",
    "[INFO] randomized search best parameters: {'clf__weights': 'uniform', 'clf__p': 1, 'clf__n_neighbors': 27, 'clf__metric': 'euclidean', 'clf__leaf_size': 25, 'clf__algorithm': 'kd_tree'}\n",
    "```\n",
    "\n",
    "Both `RandomizedSearchCV` and `GridSearchCV` require you to provide a grid of parameters.  You will need to refer to the documentation for the classifier you are using to get a list of paramenters for that particular model.  Also since we will be using the pipeline, you have to format the parameters correctly.  The name of the variable must be preceeded by the name of the step in your pipeline and two underscores.  For example.  If the classifier in the pipeline is called `clf`, and you have a tuning parameter called `metric`, the parameter grid would be as follows:\n",
    "```python\n",
    "params = {\n",
    "    \"clf__n_neighbors\": np.arange(1, 50, 2),\n",
    "    \"clf__metric\": [\"euclidean\", \"cityblock\"] \n",
    "}\n",
    "```\n",
    "\n",
    "### Your Task\n",
    "Using either GridSearchCV or RandomizedSearchCV, improve the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:40.622127Z",
     "start_time": "2020-01-21T17:47:33.723033Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\"clf__n_neighbors\": np.arange(1, 50, 2), \n",
    "         \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "         \"clf__algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "         \"clf__leaf_size\": np.arange(1, 80, 2),\n",
    "         \"clf__p\": [1,2],\n",
    "         \"clf__metric\": [\"euclidean\", \"manhattan\"]}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Your code here ...\n",
    "\n",
    " \n",
    "# evaluate the best randomized searched model on the testing data\n",
    "print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "acc = grid.best_score_\n",
    "print(\"[INFO] grid search accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "Your final task is to:\n",
    "1.  Using RandomForest, create a classifier for the DGA dataset\n",
    "2.  Use either GridSearchCV or RandomizedSearchCV to find the optimal parameters for this model.\n",
    "\n",
    "How does this model compare with the first K-NN classifier for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:47:51.330582Z",
     "start_time": "2020-01-21T17:47:40.839544Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "params = {\n",
    "    \"n_estimators\": np.arange(1, 400, 50),\n",
    "    \"max_features\": ['auto', 'sqrt','log2' ],\n",
    "    \"max_depth\": np.arange(1, 20, 2),\n",
    "    \"criterion\": ['gini','entropy']\n",
    "} \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Your code here... \n",
    "# Do the Random search on the Random Forest\n",
    "\n",
    " \n",
    "# evaluate the best randomized searched model on the testing data\n",
    "print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "acc = rf_grid.best_score_\n",
    "print(\"[INFO] grid search accuracy: {:.2f}%\".format(acc * 100))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(rf_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Everything with TPOT \n",
    "In the final step, you will use TPOT to create a classification pipeline using the DGA data set that we have been using.  The `TPOTClassifier()` has many configuration options and in the interest of time, please set the following variables when you instantiate the classifier.\n",
    "\n",
    "* `max_time_mins`:  In the interests of time, set this to 15 or 20.\n",
    "* `verbosity`: Set to 1 or 2 so you can see what TPOT is doing.\n",
    "\n",
    "\n",
    "**Note:  This step will take some time, so you might want to get some coffee or a snack when it is running.**  While this is running take a look at the other configuration options available here: http://epistasislab.github.io/tpot/api/.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:54:57.675007Z",
     "start_time": "2020-01-21T17:49:56.640864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:47:27.810498Z",
     "start_time": "2020-01-21T16:47:27.803573Z"
    }
   },
   "source": [
    "## Step Three:  Evaluate the Performance\n",
    "Now that you have a trained model, the next step is to evaluate the performance and see how TPOT did in comparison with earlier models we created.  Use the techniques you've learned to evaluate the performance of your model.  Specifically, print out the `classification report` and a confusion matrix. \n",
    "\n",
    "Unfortunately, Yellowbrick will not work in this instance, however, you can generate a similar visual confusion matrix with the following code:\n",
    "\n",
    "```\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.confusion_matrix(optimized_preds, target_test)\n",
    "\n",
    "```\n",
    "\n",
    "What is the accuracy of your model?  Is it significantly better than what you did in earlier labs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:59:49.673882Z",
     "start_time": "2020-01-21T17:59:49.655956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:59:50.742969Z",
     "start_time": "2020-01-21T17:59:50.735193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T18:00:10.096617Z",
     "start_time": "2020-01-21T18:00:09.523902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
